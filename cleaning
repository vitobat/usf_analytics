import pandas as pd
import ast
import re
from tqdm import tqdm

# Enable tqdm for pandas
tqdm.pandas()

# Load the dataset with explicit dtype to avoid DtypeWarning
file_path = 'annotated_output.csv'
print(f"Loading dataset from {file_path}...")

# Define dtypes for columns if known, else set low_memory=False
# For this example, we'll set low_memory=False
try:
    data = pd.read_csv(file_path, low_memory=False)
    print(f"Dataset loaded. Shape: {data.shape}")
except Exception as e:
    print(f"Error loading the dataset: {e}")

# --- Step 1: Inspect the Data's Head ---
print("\n--- Data Head ---")
print(data.head())

# --- Step 2: Data Cleaning ---
print("\nStarting comprehensive data cleaning...")

def clean_text(text):
    """
    Cleans the input text by:
    - Checking if the input is a string
    - Stripping leading/trailing whitespace
    - Converting to lowercase
    - Removing URLs
    - Removing all non-alphabetic characters (retain spaces and apostrophes)
    - Removing extra spaces
    """
    if not isinstance(text, str):
        return ''
    text = text.strip().lower()  # Standardize case and remove extra spaces
    text = re.sub(r"http\S+", "", text)  # Remove URLs
    text = re.sub(r"[^a-zа-яіїґє' ]+", " ", text)  # Remove special characters except @ and #
    text = re.sub(r"\s+", " ", text)  # Replace multiple spaces with single space
    return text

# Filter rows where the text column is not null and clean the text
print("\nFiltering non-null text rows...")
data = data[data['text'].notnull()].copy()
print(f"Filtered dataset shape: {data.shape}")

print("Cleaning the text data...")
data['cleaned_text'] = data['text'].progress_apply(clean_text)

# --- Step 3: Parse and Clean the Date Column ---
if 'date' in data.columns:
    print("\nParsing the date column...")
    data['date'] = pd.to_datetime(data['date'], errors='coerce')
    print("Date column parsed.")
else:
    print("\nNo 'date' column found. Skipping date parsing.")

# --- Step 4: Expand and Clean Annotations Data ---
if 'annotations' in data.columns:
    print("\nExpanding and cleaning annotations...")

    def expand_annotations(row):
        """
        Expands the 'annotations' column into separate columns:
        - Words
        - POS_Tags
        - Lemmas
        - Dependencies
        All textual data within these fields are cleaned to contain only lowercase words.
        """
        try:
            # Convert string to a Python list of dictionaries
            annotations = ast.literal_eval(row)
            words = [clean_text(item.get('Word', '')) for item in annotations]
            pos_tags = [clean_text(item.get('POS', '')) for item in annotations]
            lemmas = [clean_text(item.get('Lemma', '')) for item in annotations]
            dependencies = [clean_text(item.get('Dependency', '')) for item in annotations]
            return pd.Series([words, pos_tags, lemmas, dependencies])
        except (ValueError, SyntaxError):
            # Handle parsing errors gracefully
            return pd.Series([[], [], [], []])

    # Apply the function with a progress bar
    expanded_columns = data['annotations'].progress_apply(expand_annotations)
    expanded_columns.columns = ['Words', 'POS_Tags', 'Lemmas', 'Dependencies']

    # Combine the expanded columns with the original DataFrame
    data = pd.concat([data, expanded_columns], axis=1)
    print("Annotations expanded and cleaned.")
else:
    print("\nNo 'annotations' column found. Skipping annotations expansion.")

columns_to_remove = ["Lemmas_cleaned","Dependencies","Words","text_entities"]
data = data.drop(columns=columns_to_remove, errors='ignore')

columns_to_delete = [
                "annotations", "boosts", "reply_to_peer_id", "author",
                "invoice_information", "contact_information", "duration",
                "score", "game_message_id", "game_link", "game_description",
                "game_title", "saved_from", "inviter", "location_information",
                "inline_bot_buttons", "message_id", "via_bot", "performer", "poll", "duration_seconds", "members", "mime_type",
                "sticker_emoji", "media_type", "thumbnail", "file_name", "file", "height", "width",
                "photo", "actor", "actor_id", "action", "live_location_period_seconds"
            ]
existing_columns_to_delete = [col for col in columns_to_delete if col in data.columns]
if existing_columns_to_delete:
                data = data.drop(columns=existing_columns_to_delete)
                print(f"Deleted columns: {existing_columns_to_delete}")
else:
                print("No columns to delete from the list.")

# --- Step 5: Final Cleanup ---
print("\nPerforming final cleanup...")
output_file = 'cleaned_text_and_annotations.csv'
try:
    data.to_csv(output_file, index=False, encoding='utf-8')
    print(f"Cleaned and expanded dataset saved to '{output_file}'. Shape: {data.shape}")
except Exception as e:
    print(f"Error saving the cleaned dataset: {e}")
