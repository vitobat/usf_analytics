---
title: "Telegram Messages Analysis Report"
author: "Your Name"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
    fig_caption: true
    latex_engine: xelatex
    keep_tex: true
fontsize: 11pt
geometry: margin=1in
---

```{r}
# Load necessary libraries
library(jsonlite)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(lubridate)
library(udpipe)
library(stringr)
library(textrank)
library(wordcloud2)
library(tm)
library(textTinyR)
library(text2vec)
library(ruimtehol)
library(reticulate)
library(igraph)
library(e1071)
library(caret)
library(SnowballC)
library(quanteda)
library(quanteda.textstats)
library(nnet)
library(reshape2)
library(topicmodels)
library(broom)
library(tidytext)
library(kernlab)
library(prophet)
library(tibble)
library(hunspell)
library(scales)
library(zoo)
library(parallel)
library(factoextra)
library(readr)
library(tidyr)
studfreedom_df <- read.csv("D:/ChatExport_2025-01-02 (1)/messages.csv", stringsAsFactors = FALSE)
```

```{r}
# Data Preprocessing
studfreedom_df <- studfreedom_df %>%
  mutate(
    date = as.character(date),  # Ensure date is in character format
    normalized_datetime = ymd_hms(date),  # Normalize and parse date and time
    normalized_date = as.Date(normalized_datetime),  # Extract date part
    normalized_time = format(normalized_datetime, "%H:%M:%S")
  )
# Filter rows where the number of characters in text is >= 130
studfreedom_df <- studfreedom_df %>%
  filter(nchar(text) >= 130) # Define this number manually
# Ensure 'normalized_datetime' is of type POSIXct
studfreedom_df$normalized_datetime <- as.POSIXct(studfreedom_df$normalized_datetime)
```

```{r}
## Define the cleaning function
clean_text_function <- function(text) {
  if (!is.character(text)) {
    text <- as.character(text)
  }
  # Remove emojis and special symbols
  text <- str_replace_all(text, 
    "[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F700-\U0001F77F\U0001F780-\U0001F7FF\U0001F800-\U0001F8FF\U0001F900-\U0001F9FF\U0001FA00-\U0001FA6F\U0001FA70-\U0001FAFF\U00002702-\U000027B0\U000024C2-\U0001F251]+", "")
  # Remove URLs
  text <- str_replace_all(text, 
    "http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]{2}))+",
    "")
  # Remove specific date patterns
  text <- str_replace_all(text, "\\d{1,2}-\\d{1,2} [–∞-—è–ê-–Ø]+ \\d{4} —Ä–æ–∫—É", "")
  # Convert to lowercase
  text <- tolower(text)
  # Remove digits
  text <- str_replace_all(text, "\\d+", "")
  # Remove special characters
  text <- str_replace_all(text, "[*\\{\\}\\[\\]<>]", "")
  # Remove HTML tags
  text <- str_replace_all(text, "<[^>]*>", "")
  # Trim leading and trailing whitespaces
  text <- str_trim(text)
  # Remove newlines and paragraphs (flatten the text into a single line)
  text <- str_replace_all(text, "[\r\n]+", " ")
  return(text)
}

# Apply the cleaning function
studfreedom_df$clean_text <- sapply(studfreedom_df$text, clean_text_function)

# Load or download the UDPIPE model
model_language <- "ukrainian"
model_file <- paste0(model_language, "-ud-2.5-191206.udpipe")
if (!file.exists(model_file)) {
  udmodel <- udpipe_download_model(language = model_language)
  udmodel <- udpipe_load_model(udmodel$file_model)
} else {
  udmodel <- udpipe_load_model(model_file)
}

# Perform the annotation, handling any possible errors
annotations <- udpipe_annotate(udmodel, x = studfreedom_df$clean_text)
annotation_df <- as.data.frame(annotations)
```

```{r}
# Define identity_tokenizer
identity_tokenizer <- function(x) {
  return(x)
}

# Filter and group the data as before
filtered_data <- annotation_df %>%
  filter(upos %in% c("NOUN", "ADJ")) %>%
  group_by(doc_id, paragraph_id, sentence_id) %>%
  summarize(sentence = paste(token, collapse = " "), .groups = 'drop')

apply_textrank_to_sentence <- function(text) {
  # Skip empty or very short texts
  if (nchar(text) == 0 || length(unlist(strsplit(text, "\\s+"))) <= 1) {
    return(NULL)
  }
  
  tokens <- unlist(strsplit(text, "\\s+"))
  it <- itoken(list(tokens), tokenizer = identity_tokenizer, progressbar = FALSE)
  
  # Use tryCatch to handle errors in vocabulary creation and TCM generation
  vocab <- tryCatch(create_vocabulary(it), error = function(e) return(NULL))
  if (is.null(vocab) || length(vocab$term) == 0) return(NULL)
  
  vectorizer <- vocab_vectorizer(vocabulary = vocab)
  
  # Generate TCM and handle errors
  tcm <- tryCatch(create_tcm(it, vectorizer, skip_grams_window = 5L), error = function(e) return(NULL))
  if (is.null(tcm) || !is.matrix(tcm) || sum(tcm) == 0) return(NULL)
  
  # Ensure symmetry for the TCM
  tcm <- (tcm + t(tcm)) / 2
  
  # Generate the graph and compute PageRank
  g <- graph_from_adjacency_matrix(tcm, weighted = TRUE, mode = "undirected")
  pr <- page_rank(g, directed = FALSE)$vector
  names(pr) <- vocab$term
  return(sort(pr, decreasing = TRUE))
}

# Set up a parallel cluster
num_cores <- detectCores() - 1  # Use one less than the total cores to avoid overloading
cl <- makeCluster(num_cores)

# Export necessary libraries and objects to each worker
clusterExport(cl, c("apply_textrank_to_sentence", "identity_tokenizer", "filtered_data"))
clusterEvalQ(cl, {
  library(text2vec)
  library(igraph)
})

# Apply the function to each sentence in parallel
keywords_results <- parLapply(cl, filtered_data$sentence, apply_textrank_to_sentence)

# Stop the cluster after completion
stopCluster(cl)

# Convert results to a data frame
keywords_df <- do.call(rbind, lapply(seq_along(keywords_results), function(i) {
  if (is.null(keywords_results[[i]])) return(NULL)
  data.frame(
    SentenceID = i,
    term = names(keywords_results[[i]]),
    score = keywords_results[[i]],
    stringsAsFactors = FALSE
  )
}))

# Remove rows with NULL values
keywords_df <- na.omit(keywords_df)
```

```{r}
library(stringr)
library(dplyr)

# Define positive and negative emojis, including combined emojis
positive_emojis <- c("üî•", "üëç", "‚ù§", "‚ù§‚Äçüî•", "üòç", "üòÅ", "üíØ")      # Add more positive emojis as needed
negative_emojis <- c("üëé", "üò¢", "üò°", "üò≠", "ü§î", "üò¥")          # Add more negative emojis as needed

# Step 1: Parse the 'reactions' column to create 'reactions_summary'
studfreedom_df <- studfreedom_df %>%
  mutate(
    # Split the reactions string into individual reactions
    reactions_summary = str_split(reactions, ",\\s*") %>%
      lapply(function(x) {
        # Skip processing if the reaction string is empty
        if(length(x) == 1 && x == "") {
          return(data.frame(type = character(0), count = numeric(0), stringsAsFactors = FALSE))
        }
        # Split each reaction into emoji and count
        reactions <- str_split_fixed(x, ":", 2)
        # Trim whitespace and convert counts to numeric
        reactions_df <- data.frame(
          type = str_trim(reactions[,1]),
          count = as.numeric(str_trim(reactions[,2])),
          stringsAsFactors = FALSE
        )
        # Remove any rows with NA counts resulting from improper formatting
        reactions_df <- reactions_df %>% filter(!is.na(count))
        return(reactions_df)
      })
  )

# Step 2: Calculate positive_reactions, negative_reactions, and total_reactions
studfreedom_df <- studfreedom_df %>%
  mutate(
    # Calculate positive_reactions by summing counts of positive emojis
    positive_reactions = sapply(reactions_summary, function(df) {
      if(nrow(df) == 0) {
        return(0)
      }
      sum(df$count[df$type %in% positive_emojis], na.rm = TRUE)
    }),
    
    # Calculate negative_reactions by summing counts of negative emojis
    negative_reactions = sapply(reactions_summary, function(df) {
      if(nrow(df) == 0) {
        return(0)
      }
      sum(df$count[df$type %in% negative_emojis], na.rm = TRUE)
    }),
    
    # Calculate total_reactions as the sum of positive and negative reactions
    total_reactions = positive_reactions + negative_reactions
  )

```

```{r}
# Readability Analysis Function
readability_analysis <- function(raw_text) {
  corpus <- corpus(raw_text)
  readability_results <- textstat_readability(corpus, measure = c("ARI", "Coleman.Liau.grade", "Flesch.Kincaid", "SMOG"))
  
  readability_methods <- c("ARI", "Coleman.Liau.grade", "Flesch.Kincaid", "SMOG")
  readability_values <- readability_results[, readability_methods]
  
  return(readability_values)
}
```

```{r}
library(text2vec)

# Prepare text for similarity comparison
it <- itoken(studfreedom_df$clean_text, tokenizer = word_tokenizer)
vocab <- create_vocabulary(it)
vectorizer <- vocab_vectorizer(vocab)
dtm_similarity <- create_dtm(it, vectorizer)

# Calculate cosine similarity between texts
similarity_matrix <- sim2(dtm, method = "cosine")
```

```{r}
# Define Ukrainian conjunctions
ukrainian_conjunctions <- c(
  "—Ç–æ–º—É —â–æ", "–∞–ª–µ", "—Ç–∞", "—ñ", "–æ–¥–Ω–∞–∫", "–Ω–µ–∑–≤–∞–∂–∞—é—á–∏ –Ω–∞", 
  "–±–æ", "—Ö–æ—á", "–æ—Å–∫—ñ–ª—å–∫–∏", "—á–µ—Ä–µ–∑ —Ç–µ —â–æ", "–ø–æ–∫–∏", "–∞–±–æ", 
  "—Ç–∞–∫–æ–∂", "–Ω–∞–≤—ñ—Ç—å", "–ø—Ä–æ—Ç–µ", "–≤—Ç—ñ–º", "—Ö–æ—á–∞", "—â–æ–±", 
  "–≤–Ω–∞—Å–ª—ñ–¥–æ–∫", "—è–∫—â–æ", "—è–∫", "–Ω–µ–º–æ–≤", "–Ω—ñ–±–∏", "–∑–∞—Ç–µ", 
  "—Ç–∏–º –Ω–µ –º–µ–Ω—à", "–∞–¥–∂–µ", "—Ö–æ—á–∞ –±", "–ª–∏—à–µ", "–ª–µ–¥—å", 
  "–¥–æ–∫–∏", "–ø–æ–ø—Ä–∏", "–∑–∞—Ä–∞–¥–∏", "–∑–æ–∫—Ä–µ–º–∞", "—è–∫—â–æ –±", "—Ç–æ–º—É", 
  "–∑–∞–≤–¥—è–∫–∏", "–¥–æ —Ç–æ–≥–æ –∂", "–ø–æ–¥—ñ–±–Ω–æ", "–∑–∞ —É–º–æ–≤–∏", "—â–æ–π–Ω–æ", 
  "—Ç–∞ –π", "–¥–æ —Ä–µ—á—ñ", "–æ—Å–∫—ñ–ª—å–∫–∏", "–∑ –º–µ—Ç–æ—é", "–∑ –æ–≥–ª—è–¥—É –Ω–∞", 
  "—Ç–∏–º —á–∞—Å–æ–º —è–∫", "—Ä–∞–∑–æ–º –∑ —Ç–∏–º", "–∑–∞–º—ñ—Å—Ç—å —Ç–æ–≥–æ —â–æ–±", 
  "–∑–≤–∞–∂–∞—é—á–∏ –Ω–∞", "—Ö–æ—á –±–∏", "—É –º—ñ—Ä—É —Ç–æ–≥–æ —è–∫", "–Ω–∞ –≤–∏–ø–∞–¥–æ–∫ —è–∫—â–æ"
)

# Function to count conjunctions in text
cohesion_analysis <- function(text) {
  sapply(ukrainian_conjunctions, function(word) sum(str_count(text, fixed(word, ignore_case = TRUE))))
}

# Apply to all texts
cohesion_counts <- lapply(studfreedom_df$clean_text, cohesion_analysis)

# Convert to data frame
cohesion_df <- do.call(rbind, cohesion_counts)
colnames(cohesion_df) <- ukrainian_conjunctions
```

```{r}

sentiment_path <- "sentiment_dictionary.csv"
sentiment_dictionary <- read_csv(sentiment_path, show_col_types = FALSE)

# Rename columns for clarity if necessary
# Assuming the sentiment dictionary has columns 'X1' and 'new_col'
# We will rename them to 'word' and 'sentiment_score'
sentiment_dictionary <- sentiment_dictionary %>%
  rename(word = X1, sentiment_score = new_col)

# Check the structure of the sentiment dictionary
print(head(sentiment_dictionary))

# Sentiment analysis function
sentiment_analysis <- function(sentiment_dictionary, preprocessed_text) {
  # Split the text into words
  text_as_wordlist <- unlist(str_split(preprocessed_text, pattern = "\\s+"))
  text_length <- length(text_as_wordlist)
  
  # Create a data frame from the word list
  text_df <- data.frame(word = text_as_wordlist, stringsAsFactors = FALSE)
  
  # Match words with sentiment dictionary
  sentiment_df <- left_join(text_df, sentiment_dictionary, by = "word")
  
  # Replace NA sentiment scores with 0 (neutral sentiment)
  sentiment_df$sentiment_score[is.na(sentiment_df$sentiment_score)] <- 0
  
  # Calculate counts for each sentiment score
  sentiment_counts <- sentiment_df %>%
    group_by(sentiment_score) %>%
    summarise(count = n())
  
  # Initialize sentiment values
  sentiment_values <- c(
    minus_twos = 0,
    minus_ones = 0,
    ones = 0,
    twos = 0
  )
  
  # Fill in the sentiment values
  sentiment_values["minus_twos"] <- sum(sentiment_df$sentiment_score == -2) / text_length
  sentiment_values["minus_ones"] <- sum(sentiment_df$sentiment_score == -1) / text_length
  sentiment_values["ones"] <- sum(sentiment_df$sentiment_score == 1) / text_length
  sentiment_values["twos"] <- sum(sentiment_df$sentiment_score == 2) / text_length
  
  return(sentiment_values)
}

# Apply the sentiment analysis function to each row in the data frame
# Ensure 'studfreedom_df' has a column 'clean_text'
sentiment_results <- t(sapply(studfreedom_df$clean_text, function(text) {
  sentiment_analysis(sentiment_dictionary, text)
}))

# Convert the results into a data frame
sentiment_df <- as.data.frame(sentiment_results)
rownames(sentiment_df) <- NULL

# Combine the sentiment scores with the original data frame
studfreedom_df <- cbind(studfreedom_df, sentiment_df)

overall_sentiment_summary <- studfreedom_df %>%
  summarise(
    minus_twos_total = sum(minus_twos, na.rm = TRUE),
    minus_ones_total = sum(minus_ones, na.rm = TRUE),
    ones_total = sum(ones, na.rm = TRUE),
    twos_total = sum(twos, na.rm = TRUE)
  )

# Convert the summary into a long format for easier plotting and analysis
sentiment_long <- overall_sentiment_summary %>%
  pivot_longer(cols = everything(), names_to = "sentiment", values_to = "total")

# Map sentiment levels to descriptive emotion labels
sentiment_long <- sentiment_long %>%
  mutate(
    sentiment_label = case_when(
      sentiment == "minus_twos_total" ~ "Very Negative",
      sentiment == "minus_ones_total" ~ "Negative",
      sentiment == "ones_total" ~ "Positive",
      sentiment == "twos_total" ~ "Very Positive"
    )
  )

# Calculate the percentage of each sentiment
total_sentiment <- sum(sentiment_long$total)
sentiment_long <- sentiment_long %>%
  mutate(percentage = (total / total_sentiment) * 100)

# Get the top emotions (in this case, all emotions since we have 4 levels)
top_emotions <- sentiment_long %>%
  arrange(desc(percentage))
```

```{r}
# Readability Analysis
readability_results <- lapply(studfreedom_df$text, readability_analysis)

# Convert the list of results into a data frame
readability_df <- do.call(rbind, readability_results)

# Combine the readability scores with the original data frame
studfreedom_df <- cbind(studfreedom_df, readability_df)
```

```{r}
ukrainian_stopwords <- c(
  "–∞–ª–µ", "–∞", "–±", "–±–µ–∑", "–±–∏", "–±—É–≤", "–±—É–¥–µ", "–±—É–ª–∏", "–±—É—Ç–∏",
  "–≤", "–≤–∞–º", "–≤–∞—Å", "–≤–µ—Å—å", "–≤–∏", "–≤—ñ–Ω", "–≤–æ–Ω–∏", "–≤–æ–Ω–æ", "–≤–æ–Ω–∞", "–≤–∏—â–µ", "–≤–∂–µ",
  "–¥–µ", "–¥–ª—è", "–¥–æ", "–∂", "–∑", "–∑–∞", "—ñ", "–π", "—ó—ó", "–π–æ–≥–æ", "—ñ–∑", "—ó—Ö", "—ó—Ö–Ω—ñ",
  "—ñ–∑", "—ñ–Ω—à–∏—Ö", "–π–æ–≥–æ", "–∫–∞–∂–µ", "–∫–æ–ª–∏", "–∫–æ–∂–µ–Ω",
  "–∫—Ä—ñ–º", "–∫—É–¥–∏", "–ª–∏—à–µ", "–º–∞–π–∂–µ", "–º–∏", "–º–µ–Ω—ñ", "–º—ñ–π",
  "–º–æ–∂–Ω–∞", "–º–æ—è", "–º–æ—î", "–º–æ—ó", "–Ω–∞", "–Ω–∞–¥", "–Ω–∞–º", "–Ω–∞–º–∏", "–Ω–∞—Å", "–Ω–∞—à–µ", "–Ω–µ",
  "–Ω—ñ", "–Ω—ñ–±–∏", "–Ω–∏—Ö", "–Ω—É", "–æ", "–æ–±", "–æ–¥–∏–Ω", "–æ–¥–Ω–∞", "–æ–¥–Ω—ñ", "–æ–¥–Ω–µ", "–æ–¥–Ω–æ–≥–æ",
  "–æ—Å—å", "–æ—Ç", "–ø–æ", "–ø—Ä–∏", "–ø—Ä–æ", "—Å", "—Å–∞–º", "—Å–∞–º–∞", "—Å–∞–º—ñ", "—Å–≤–æ—ó", "—Å–≤–æ–≥–æ",
  "—Å–≤–æ—î", "—Å–µ–±–µ", "—Å–æ–±—ñ", "—Ç–∞", "—Ç–∞–∫", "—Ç–∞–∫–∏–π", "—Ç–∞–∫–∏–º", "—Ç–∞–∫–æ–∂", "—Ç–µ–ø–µ—Ä", "—Ç–µ",
  "—Ç–∏–º", "—Ç–∏—Å—è—á–∞", "—Ç–æ–π", "—Ç–æ", "—Ç—Ä–µ–±–∞", "—Ç–∏—Ö", "—Ç–æ–π", "—Ç–æ–π", "—Ç—ñ", "—Ç—ñ–º", "—Ç—ñ—î—ó",
  "—Ç–∏—Ö", "—É", "—É—Å–µ", "—É—Å—ñ—Ö", "—É–∂–µ", "—Ü–µ", "—Ü–µ–π", "—Ü—è", "—Ü—ñ", "—Ü—ñ—î—ó", "—Ü—è", "—Ü–µ–π",
  "—Ü—è", "—Ü—ñ—î—ó", "—Ü—å–æ–≥–æ", "—Ü—å–æ–º—É", "—Ü–µ", "—Ü–µ–π", "—Ü—è", "—Ü–µ", "—â–æ", "—â–æ–±", "—è–∫",
  "—è–∫–∏–π", "—è–∫—ñ", "—è–∫–∏—Ö", "—è–∫–∏–º–∏", "—è–∫–∏—Ö", "—è–∫—â–æ", "—ó—ó", "—ó—Ö–Ω—ñ–π", "—ó—Ö–Ω—å–æ–≥–æ", "–∑—ñ","–≤—ñ–¥", "—î", "—â–µ",
  "—è", "—á–∏", "–ø—ñ–¥", "—Ç–æ–º—É", "—á–æ–º—É"
)
```

```{r}
# Step 2: Calculate the 75th percentile of positive_reactions
percentile_75 <- quantile(studfreedom_df$positive_reactions, 0.75, na.rm = TRUE)
print(paste("75th Percentile of Positive Reactions:", percentile_75))

# Step 3: Filter posts with positive_reactions >= 75th percentile
top_positive_posts <- studfreedom_df %>%
  filter(positive_reactions >= percentile_75)

print(paste("Number of Top Positive Posts:", nrow(top_positive_posts)))

# Step 4: Annotate text using udpipe and extract nouns per post
top_positive_posts_annotated <- udpipe_annotate(udmodel, x = top_positive_posts$clean_text)
top_positive_posts_annotated_df <- as.data.frame(top_positive_posts_annotated)

top_positive_posts_nouns <- top_positive_posts_annotated_df %>%
  filter(upos == "NOUN") %>%  # Keep only nouns
  group_by(doc_id) %>%         # Group by post ID instead of sentence_id
  summarize(text = paste(lemma, collapse = " "), .groups = 'drop')  # Combine lemmas into a single text per post

# Step 5: Prepare text data for LDA
tidy_text <- top_positive_posts_nouns %>%
  select(doc_id, text) %>%           
  unnest_tokens(word, text) %>%                   
  filter(!word %in% ukrainian_stopwords) %>%            # Remove Ukrainian stopwords
  filter(!str_detect(word, "^\\d+$"))                   # Remove standalone numbers

# Step 6: Create Document-Term Matrix
dtm <- tidy_text %>%
  count(doc_id, word, sort = TRUE) %>%
  cast_dtm(document = doc_id, term = word, value = n)

# Check if DTM is not empty
if (dim(dtm)[1] == 0 || dim(dtm)[2] == 0) {
  stop("Document-Term Matrix is empty. Check your preprocessing steps.")
}

# Step 7: Perform LDA to Identify Topics
num_topics <- 15  # You can adjust this number based on your data

set.seed(1234)  # For reproducibility
lda_model <- LDA(dtm, k = num_topics, control = list(seed = 1234))

# Step 8: Extract Topics and Top Terms
topics <- tidy(lda_model, matrix = "beta")

top_terms <- topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

print(top_terms)

# Step 9: Assign Topics to Documents
doc_topics <- tidy(lda_model, matrix = "gamma") %>%
  group_by(document) %>%
  top_n(1, gamma) %>%
  ungroup()

doc_topics$document <- as.numeric(gsub("[^0-9]", "", doc_topics$document))
doc_topics$document <- as.integer(doc_topics$document)

# Merge the topic assignments back to the top_positive_posts dataframe
# Ensure that 'doc_id' corresponds to 'id' in top_positive_posts
top_positive_posts <- top_positive_posts %>%
  arrange(id) %>%  # Optional: Arrange by 'id' or any other relevant column
  mutate(document = row_number()) %>%  
 left_join(doc_topics, by = "document")

# Step 10: Aggregate Positive Reactions by Topic
topic_reactions <- top_positive_posts %>%
  group_by(topic) %>%
  summarise(
    total_positive_reactions = sum(positive_reactions, na.rm = TRUE),
    count = n()
  ) %>%
  arrange(desc(total_positive_reactions))

print(topic_reactions)

# Step 11: Visualize the Most Liked Topics
ggplot(topic_reactions, aes(x = factor(topic), y = total_positive_reactions, fill = factor(topic))) +
  geom_bar(stat = "identity") +
  labs(
    title = "Most Liked Topics Based on Positive Reactions",
    x = "Topic",
    y = "Total Positive Reactions"
  ) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_fill_brewer(palette = "Set3")
```

```{r}
# N-gram Analysis

# Step 1: Create Corpus from Text Data
grouped_tokens <- annotation_df %>%
  filter(upos == "NOUN") %>%  # Keep only nouns
  group_by(sentence_id) %>%    # Group by sentence ID
  summarize(text = paste(lemma, collapse = " "), .groups = 'drop')  # Combine lemmas into a single sentence

# Step 2: Remove Ukrainian Stopwords
grouped_tokens <- grouped_tokens %>%
  mutate(text = gsub(paste0("\\b(", paste(ukrainian_stopwords, collapse = "|"), ")\\b"), "", text))

# Step 3: Create and Clean a Text Corpus
# Create a corpus
corpus_LDA <- Corpus(VectorSource(grouped_tokens$text))

# Clean the text
corpus_LDA <- tm_map(corpus_LDA, content_transformer(tolower))
corpus_LDA <- tm_map(corpus_LDA, removePunctuation)
corpus_LDA <- tm_map(corpus_LDA, removeNumbers)
corpus_LDA <- tm_map(corpus_LDA, stripWhitespace)

# Step 4: Create a Document-Term Matrix (DTM)
# Create Document-Term Matrix
dtm_LDA <- DocumentTermMatrix(corpus_LDA)

# Remove sparse terms
dtm_LDA <- removeSparseTerms(dtm_LDA, 0.95)

# Check and remove empty rows
row_totals <- apply(dtm_LDA, 1, sum)
dtm_LDA <- dtm_LDA[row_totals > 0, ]

# Step 5: Perform LDA
# Set the number of topics
k <- 10

# Fit LDA model
lda_model_2 <- LDA(dtm_LDA, k = k, control = list(seed = 123))

# Step 6: Extract Topics
# Extract topics
topics <- tidy(lda_model_2, matrix = "beta")

# Get the top terms for each topic
top_terms_2 <- topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
```

```{r}
# Step 1: Create Corpus from Text Data
corpus <- corpus(studfreedom_df$clean_text)

# Step 2: Tokenization with N-grams and Stopword Removal
tokens_ngrams <- tokens(corpus, ngrams = 2:3, remove_punct = TRUE) %>%
  tokens_remove(pattern = ukrainian_stopwords)  # Remove stopwords

# Step 3: Create Document-Feature Matrix (DFM) for N-grams
dfm_ngrams <- dfm(tokens_ngrams)

# Step 4: View Top N-grams by Frequency
top_ngrams <- textstat_frequency(dfm_ngrams, n = 20)  # Top-20 n-grams
print(top_ngrams)

# Step 5: Visualization of Top N-grams
top_ngrams_for_plot <- top_ngrams %>%
  arrange(desc(frequency)) %>%
  head(20)  # Top-20 n-grams for plotting

ggplot(top_ngrams_for_plot, aes(x = reorder(feature, frequency), y = frequency)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(
    title = "Top 20 Most Frequent N-grams",
    x = "N-gram",
    y = "Frequency"
  ) +
  theme_minimal()
```

```{r}
# Temporal Analysis

# Ensure 'normalized_datetime' is of type POSIXct
studfreedom_df$normalized_datetime <- as.POSIXct(studfreedom_df$normalized_datetime)

# Add columns with date and time details
time_analysis <- studfreedom_df %>%
  mutate(
    date = as.Date(normalized_datetime),
    year = year(normalized_datetime),
    month = month(normalized_datetime, label = TRUE, abbr = FALSE),
    day = day(normalized_datetime),
    weekday = wday(normalized_datetime, label = TRUE, abbr = FALSE, week_start = 1),
    hour = hour(normalized_datetime)
  )

# Aggregate number of posts by date
posts_per_day <- time_analysis %>%
  group_by(date) %>%
  summarise(posts_count = n())

# Aggregate number of posts by month
posts_per_month <- time_analysis %>%
  group_by(year, month) %>%
  summarise(posts_count = n())

# Aggregate number of posts by weekday
posts_per_weekday <- time_analysis %>%
  group_by(weekday) %>%
  summarise(posts_count = n())

# Aggregate number of posts by hour
posts_per_hour <- time_analysis %>%
  group_by(hour) %>%
  summarise(posts_count = n())

# Create heatmap data for activity by weekday and hour
posts_heatmap <- time_analysis %>%
  group_by(weekday, hour) %>%
  summarise(posts_count = n())

# Ensure 'weekday' is a factor with correct order
posts_heatmap$weekday <- factor(posts_heatmap$weekday, levels = c("–ü–æ–Ω–µ–¥—ñ–ª–æ–∫", "–í—ñ–≤—Ç–æ—Ä–æ–∫", "–°–µ—Ä–µ–¥–∞", "–ß–µ—Ç–≤–µ—Ä", "–ü'—è—Ç–Ω–∏—Ü—è", "–°—É–±–æ—Ç–∞", "–ù–µ–¥—ñ–ª—è"))
```

```{r}
# Narrative Structure Analysis

# Calculate average sentence and paragraph lengths
narrative_structure_df <- annotation_df %>%
  group_by(doc_id) %>%
  summarise(
    num_sentences = n_distinct(sentence_id),
    num_paragraphs = n_distinct(paragraph_id),
    avg_sentence_length = n() / num_sentences,
    avg_paragraph_length = n() / num_paragraphs
  )

# Part 2: POS Frequency by Section
# Define sections: beginning, middle, end
annotation_df <- annotation_df %>%
  group_by(doc_id) %>%
  mutate(
    position = case_when(
      row_number() <= n() * 0.33 ~ "beginning",
      row_number() <= n() * 0.66 ~ "middle",
      TRUE ~ "end"
    )
  ) %>%
  ungroup()

# Calculate POS frequency for each section
pos_section_df <- annotation_df %>%
  group_by(doc_id, position, upos) %>%
  summarise(count = n()) %>%
  group_by(doc_id, position) %>%
  mutate(freq = count / sum(count)) %>%
  select(doc_id, position, upos, freq) %>%
  pivot_wider(names_from = upos, values_from = freq, values_fill = 0)

# Part 3: POS Transitions
# Calculate transitions between POS tags
pos_transitions <- annotation_df %>%
  arrange(doc_id, paragraph_id, sentence_id, token_id) %>%
  group_by(doc_id) %>%
  mutate(next_upos = lead(upos)) %>%
  filter(!is.na(next_upos)) %>%
  count(upos, next_upos) %>%
  group_by(upos) %>%
  mutate(transition_prob = n / sum(n)) %>%
  ungroup() %>%
  select(doc_id, upos, next_upos, transition_prob)

# Combine all narrative analysis results
narrative_analysis_results <- list(
  narrative_structure = narrative_structure_df,
  pos_frequency_by_section = pos_section_df,
  pos_transitions = pos_transitions
)
```

```{r}
# Pacing Metrics

# Calculate average words per post
studfreedom_df <- studfreedom_df %>%
  mutate(
    word_count = str_count(text, "\\w+")
  )

# Calculate reading time (assuming 200 words per minute)
studfreedom_df <- studfreedom_df %>%
  mutate(
    reading_time = word_count / 200
  )

# Categorize reading time
studfreedom_df <- studfreedom_df %>%
  mutate(
    reading_time_group = case_when(
      reading_time <= 1 ~ "–ú–µ–Ω—à–µ 1 —Ö–≤–∏–ª–∏–Ω–∏",
      reading_time > 1 & reading_time <= 2 ~ "1-2 —Ö–≤–∏–ª–∏–Ω–∏",
      reading_time > 2 & reading_time <= 3 ~ "2-3 —Ö–≤–∏–ª–∏–Ω–∏",
      TRUE ~ "–ë—ñ–ª—å—à–µ 3 —Ö–≤–∏–ª–∏–Ω"
    )
  )

# Analyze engagement by reading time group
engagement_by_reading_time <- studfreedom_df %>%
  group_by(reading_time_group) %>%
  summarise(
    total_engagement = sum(engagement_metric, na.rm = TRUE),
    total_posts = n(),
    avg_engagement_per_post = total_engagement / total_posts
  ) %>%
  ungroup()

# Analyze engagement by word count group
studfreedom_df <- studfreedom_df %>%
  mutate(
    word_count_group = case_when(
      word_count <= 50 ~ "–î–æ 50 —Å–ª—ñ–≤",
      word_count > 50 & word_count <= 100 ~ "51-100 —Å–ª—ñ–≤",
      word_count > 100 & word_count <= 200 ~ "101-200 —Å–ª—ñ–≤",
      TRUE ~ "–ü–æ–Ω–∞–¥ 200 —Å–ª—ñ–≤"
    )
  )

engagement_by_word_count <- studfreedom_df %>%
  group_by(word_count_group) %>%
  summarise(
    total_engagement = sum(engagement_metric, na.rm = TRUE),
    total_posts = n(),
    avg_engagement_per_post = total_engagement / total_posts
  ) %>%
  ungroup()
```

```{r}
## Engagement Rate Analysis

# Since 'views' is absent, engagement rate is based on total reactions
studfreedom_df <- studfreedom_df %>%
  mutate(
    engagement_rate = engagement_metric  # Engagement rate defined as total reactions per post
  ) %>%
  filter(!is.na(engagement_rate) & is.finite(engagement_rate))

# Statistical description
library(pastecs)
stat.desc(studfreedom_df$engagement_rate)

# Histogram with density
ggplot(studfreedom_df, aes(x = engagement_rate)) +
  geom_histogram(aes(y = ..density..), bins = 50, fill = "orange", color = "black", alpha = 0.7) +
  geom_density(color = "blue", size = 1) +
  labs(title = "–†–æ–∑–ø–æ–¥—ñ–ª –≤—ñ–¥–Ω–æ—à–µ–Ω–Ω—è –∑–∞–ª—É—á–µ–Ω–æ—Å—Ç—ñ –¥–æ —Ä–µ–∞–∫—Ü—ñ–π", x = "–ó–∞–ª—É—á–µ–Ω—ñ—Å—Ç—å (–†–µ–∞–∫—Ü—ñ—ó)", y = "–©—ñ–ª—å–Ω—ñ—Å—Ç—å") +
  theme_minimal()

# Boxplot for outliers
ggplot(studfreedom_df, aes(y = engagement_rate)) +
  geom_boxplot(fill = "lightblue") +
  labs(title = "Boxplot –≤—ñ–¥–Ω–æ—à–µ–Ω–Ω—è –∑–∞–ª—É—á–µ–Ω–æ—Å—Ç—ñ –¥–æ —Ä–µ–∞–∫—Ü—ñ–π", y = "–ó–∞–ª—É—á–µ–Ω—ñ—Å—Ç—å (–†–µ–∞–∫—Ü—ñ—ó)") +
  theme_minimal()

# Detect and remove outliers (1.5*IQR)
Q1 <- quantile(studfreedom_df$engagement_rate, 0.25)
Q3 <- quantile(studfreedom_df$engagement_rate, 0.75)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR

outliers <- studfreedom_df %>%
  filter(engagement_rate < lower_bound | engagement_rate > upper_bound)

# Remove outliers
studfreedom_df <- studfreedom_df %>%
  filter(engagement_rate >= lower_bound & engagement_rate <= upper_bound)

# Re-plot histogram without outliers
ggplot(studfreedom_df, aes(x = engagement_rate)) +
  geom_histogram(bins = 50, fill = "green", color = "black") +
  labs(title = "–†–æ–∑–ø–æ–¥—ñ–ª –≤—ñ–¥–Ω–æ—à–µ–Ω–Ω—è –∑–∞–ª—É—á–µ–Ω–æ—Å—Ç—ñ –¥–æ —Ä–µ–∞–∫—Ü—ñ–π (–±–µ–∑ –≤–∏–∫–∏–¥—ñ–≤)", x = "–ó–∞–ª—É—á–µ–Ω—ñ—Å—Ç—å (–†–µ–∞–∫—Ü—ñ—ó)", y = "–ß–∞—Å—Ç–æ—Ç–∞") +
  theme_minimal()

# Scatter plot: Engagement vs. Reactions
ggplot(studfreedom_df, aes(x = engagement_rate, y = engagement_rate)) +  # Since no views, x and y are same
  geom_point(alpha = 0.5) +
  labs(title = "–ê–Ω–∞–ª—ñ–∑ –∑–∞–ª—É—á–µ–Ω–æ—Å—Ç—ñ", x = "–ó–∞–ª—É—á–µ–Ω—ñ—Å—Ç—å (–†–µ–∞–∫—Ü—ñ—ó)", y = "–ó–∞–ª—É—á–µ–Ω—ñ—Å—Ç—å (–†–µ–∞–∫—Ü—ñ—ó)") +
  theme_minimal()

# Since x and y are the same, the plot will be a diagonal line. Adjusting to meaningful comparison:
# For example, plotting weighted engagement vs. total reactions

ggplot(studfreedom_df, aes(x = total_reactions, y = weighted_engagement)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", color = "red") +
  labs(title = "–ó–∞–ª–µ–∂–Ω—ñ—Å—Ç—å –º—ñ–∂ –∑–∞–≥–∞–ª—å–Ω–∏–º–∏ —Ä–µ–∞–∫—Ü—ñ—è–º–∏ —Ç–∞ –∑–≤–∞–∂–µ–Ω–æ—é –∑–∞–ª—É—á–µ–Ω—ñ—Å—Ç—é", x = "–ó–∞–≥–∞–ª—å–Ω—ñ —Ä–µ–∞–∫—Ü—ñ—ó", y = "–ó–≤–∞–∂–µ–Ω–∞ –∑–∞–ª—É—á–µ–Ω—ñ—Å—Ç—å") +
  theme_minimal()

# Regression model
model <- lm(weighted_engagement ~ total_reactions, data = studfreedom_df)
summary(model)

# Correlation
correlation <- cor(studfreedom_df$total_reactions, studfreedom_df$weighted_engagement, method = "pearson")
print(paste("–ö–æ—Ä–µ–ª—è—Ü—ñ—è –º—ñ–∂ –∑–∞–≥–∞–ª—å–Ω–∏–º–∏ —Ä–µ–∞–∫—Ü—ñ—è–º–∏ —Ç–∞ –∑–≤–∞–∂–µ–Ω–æ—é –∑–∞–ª—É—á–µ–Ω—ñ—Å—Ç—é:", round(correlation, 3)))

# Reading Time Analysis
# Already calculated 'reading_time' and 'reading_time_group'

# Engagement by Reading Time Group
ggplot(engagement_by_reading_time, aes(x = reading_time_group, y = avg_engagement_per_post, fill = reading_time_group)) +
  geom_bar(stat = "identity", color = "black", width = 0.6) +
  geom_errorbar(aes(ymin = avg_engagement_per_post - sd_engagement, ymax = avg_engagement_per_post + sd_engagement),
                width = 0.2, color = "black") +
  labs(
    title = "–°–µ—Ä–µ–¥–Ω—è –∑–∞–ª—É—á–µ–Ω—ñ—Å—Ç—å –Ω–∞ –ø–æ—Å—Ç –∑–∞ –≥—Ä—É–ø–∞–º–∏ —á–∞—Å—É —á–∏—Ç–∞–Ω–Ω—è",
    x = "–ì—Ä—É–ø–∞ —á–∞—Å—É —á–∏—Ç–∞–Ω–Ω—è",
    y = "–°–µ—Ä–µ–¥–Ω—è –∑–∞–ª—É—á–µ–Ω—ñ—Å—Ç—å"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    legend.position = "none"
  )

# Engagement by Word Count Group
ggplot(engagement_by_word_count, aes(x = word_count_group, y = avg_engagement_per_post, fill = word_count_group)) +
  geom_bar(stat = "identity", color = "black", width = 0.6) +
  geom_errorbar(aes(ymin = avg_engagement_per_post - sd_engagement, ymax = avg_engagement_per_post + sd_engagement),
                width = 0.2, color = "black") +
  labs(
    title = "–°–µ—Ä–µ–¥–Ω—è –∑–∞–ª—É—á–µ–Ω—ñ—Å—Ç—å –Ω–∞ –ø–æ—Å—Ç –∑–∞ –≥—Ä—É–ø–∞–º–∏ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ —Å–ª—ñ–≤",
    x = "–ì—Ä—É–ø–∞ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ —Å–ª—ñ–≤",
    y = "–°–µ—Ä–µ–¥–Ω—è –∑–∞–ª—É—á–µ–Ω—ñ—Å—Ç—å"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    legend.position = "none"
  )

```

```{r}
## Summary Metrics

# 1. General Engagement Metrics
total_posts <- nrow(studfreedom_df)
average_reactions <- mean(studfreedom_df$total_reactions, na.rm = TRUE)
average_weighted_engagement <- mean(studfreedom_df$weighted_engagement, na.rm = TRUE)
overall_engagement_total <- sum(studfreedom_df$total_reactions, na.rm = TRUE)

# 2. Temporal Analysis - Best Hours for Posting
top_hours <- posts_per_hour %>%
  arrange(desc(posts_count)) %>%
  slice(1:3)

# 3. Content Length and Reading Time
average_word_count <- mean(studfreedom_df$word_count, na.rm = TRUE)

# 4. Engagement Rate Distribution
median_engagement_rate <- median(studfreedom_df$engagement_rate, na.rm = TRUE)

# 5. Correlation Between Total Reactions and Weighted Engagement
correlation <- cor(studfreedom_df$total_reactions, studfreedom_df$weighted_engagement, method = "pearson")

# 6. Pronoun Usage
top_pronouns <- pronoun_usage %>%
  group_by(lemma) %>%
  summarise(total_count = sum(count)) %>%
  arrange(desc(total_count)) %>%
  slice(1:5)

# 7. Emotional Language Summary
emotional_language_summary <- paste0(
  "**Emotional Language:**\n",
  "The posts are predominantly ", top_emotions$sentiment_label[1], 
  ", making up ", round(top_emotions$percentage[1], 2), "% of the emotional content.\n",
  "Other significant sentiments include ", top_emotions$sentiment_label[2], 
  " (", round(top_emotions$percentage[2], 2), "%) and ", top_emotions$sentiment_label[3],
  " (", round(top_emotions$percentage[3], 2), "%)."
)

# 8. Voice in Writing
voice_usage <- overall_voice_summary %>%
  mutate(
    percentage = round(percentage, 2)
  )

# 9. Stylistic Features
average_sentence_length <- mean(narrative_structure_df$avg_sentence_length, na.rm = TRUE)
type_token_ratio <- length(unique(annotation_df$token)) / nrow(annotation_df)

# Generate Summary
summary_text <- paste0(
  "### Summary of the Telegram Channel\n\n",
  "**Total Posts Analyzed:** ", total_posts, "\n",
  "**Average Reactions per Post:** ", round(average_reactions, 2), "\n",
  "**Average Weighted Engagement per Post:** ", round(average_weighted_engagement, 2), "\n",
  "**Overall Total Engagement:** ", overall_engagement_total, "\n\n",
  "**Best Hours for Posting:**\n",
  paste0("- ", top_hours$hour, ": ", top_hours$posts_count, " posts\n", collapse = ""),
  "\n**Average Word Count per Post:** ", round(average_word_count, 2), "\n",
  "**Median Engagement Rate:** ", round(median_engagement_rate, 4), "\n",
  "**Correlation Between Total Reactions and Weighted Engagement:** ", round(correlation, 3), "\n\n",
  emotional_language_summary, "\n\n",
  "**Voice Usage in Text:**\n",
  paste0("- ", voice_usage$voice, ": ", voice_usage$percentage, "%\n", collapse = ""),
  "\n**Stylistic Features:**\n",
  "- Average Sentence Length: ", round(average_sentence_length, 2), " words\n",
  "- Type-Token Ratio: ", round(type_token_ratio, 2), "\n"
)

cat(summary_text)

```

```{r}
# Word Cloud of Most Frequent Lemmas

library(wordcloud2)

# Extract lemmas and filter by Parts of Speech
lemmas_filtered <- annotation_df %>%
  filter(upos %in% c("NOUN", "ADJ")) %>%  # Focus on nouns and adjectives
  pull(lemma) %>%                           # Extract the lemma column
  tolower() %>%                             # Convert to lowercase for consistency
  str_replace_all("[^–∞-—è—ñ—ó“ë—î—å']+", " ") %>% # Remove non-Cyrillic characters
  str_trim()                                # Trim whitespace

# Remove stopwords
lemmas_clean <- lemmas_filtered %>%
  str_split("\\s+") %>%  # Split into individual words
  unlist() %>%           # Flatten the list
  .[!. %in% ukrainian_stopwords]  # Remove stopwords

# Create a frequency table
word_freq <- table(lemmas_clean) %>%
  as.data.frame(stringsAsFactors = FALSE) %>%
  rename(word = lemmas_clean, freq = Freq) %>%
  arrange(desc(freq)) %>%
  top_n(100, freq)  # Select the top 100 most frequent words

# Generate the word cloud
wordcloud2(word_freq, size = 1, color = 'random-light', backgroundColor = "white")

```

```{r}
# Sentiment Distribution Pie Chart
ggplot(sentiment_long, aes(x = "", y = percentage, fill = sentiment_label)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar(theta = "y") +
  labs(title = "–†–æ–∑–ø–æ–¥—ñ–ª —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç—ñ–≤") +
  theme_void() +
  theme(legend.title = element_blank())

```

```{r}
# Readability Scores Distribution Boxplot
library(ggplot2)
library(tidyr)

readability_long <- readability_df %>%
  gather(key = "measure", value = "value", ARI, `Coleman.Liau.grade`, `Flesch.Kincaid`, SMOG)

ggplot(readability_long, aes(x = measure, y = value, fill = measure)) +
  geom_boxplot() +
  labs(title = "–†–æ–∑–ø–æ–¥—ñ–ª –ø–æ–∫–∞–∑–Ω–∏–∫—ñ–≤ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—ñ", x = "–ü–æ–∫–∞–∑–Ω–∏–∫ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—ñ", y = "–ë–∞–ª–∏") +
  theme_minimal() +
  theme(legend.position = "none")

```

```{r}
# Flesch-Kincaid Readability Score Histogram
ggplot(readability_df, aes(x = Flesch.Kincaid)) +
  geom_histogram(binwidth = 1, fill = "darkgreen", color = "black", alpha = 0.7) +
  labs(title = "–†–æ–∑–ø–æ–¥—ñ–ª –ø–æ–∫–∞–∑–Ω–∏–∫—ñ–≤ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—ñ Flesch-Kincaid", x = "Flesch-Kincaid Score", y = "–ß–∞—Å—Ç–æ—Ç–∞") +
  theme_minimal()
```

```{r}
# Top Terms per LDA Topic
library(ggplot2)

ggplot(top_terms_2, aes(x = reorder(term, beta), y = beta, fill = factor(topic))) +
  geom_bar(stat = "identity") +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  labs(title = "–¢–æ–ø —Ç–µ—Ä–º—ñ–Ω—ñ–≤ –∑–∞ —Ç–µ–º–∞–º–∏ LDA", x = "–¢–µ—Ä–º—ñ–Ω", y = "–ë–µ—Ç–∞") +
  theme_minimal() +
  theme(legend.position = "none")

```

```{r}
# ANOVA Function for Significance Testing
significance_result <- function(anova_result) {
  # Extract the ANOVA table directly using `anova()`
  anova_table <- anova(anova_result)
  
  # Check if "Pr(>F)" column exists
  if ("Pr(>F)" %in% colnames(anova_table)) {
    # Extract the first p-value
    p_value <- anova_table$`Pr(>F)`[1]
    
    # Check if p-value is significant
    if (!is.na(p_value) && p_value < 0.05) {
      cat("Significant Result: There is a significant difference based on the factor.\n")
      cat("p-value:", p_value, "\n")
    } else {
      cat("Not Significant: No significant difference was found based on the factor.\n")
      cat("p-value:", p_value, "\n")
    }
  } else {
    cat("Error: The ANOVA result does not have a 'Pr(>F)' column.\n")
  }
}
```

```{r}
# Engagement Metrics Summary

# 1. General Engagement Metrics
total_posts <- nrow(studfreedom_df)
average_reactions <- mean(studfreedom_df$total_reactions, na.rm = TRUE)
average_weighted_engagement <- mean(studfreedom_df$weighted_engagement, na.rm = TRUE)
overall_engagement_total <- sum(studfreedom_df$total_reactions, na.rm = TRUE)

# 2. Temporal Analysis - Best Hours for Posting
# Already computed as 'top_hours'

# 3. Content Length and Reading Time
average_word_count <- mean(studfreedom_df$word_count, na.rm = TRUE)

# 4. Engagement Rate Distribution
median_engagement_rate <- median(studfreedom_df$engagement_rate, na.rm = TRUE)

# 5. Correlation Between Total Reactions and Weighted Engagement
correlation <- cor(studfreedom_df$total_reactions, studfreedom_df$weighted_engagement, method = "pearson")

# 6. Pronoun Usage - Top 5 Pronouns
top_pronouns <- pronoun_usage %>%
  group_by(lemma) %>%
  summarise(total_count = sum(count)) %>%
  arrange(desc(total_count)) %>%
  slice(1:5)

# 7. Emotional Language Summary
emotional_language_summary <- paste0(
  "**Emotional Language:**\n",
  "The posts are predominantly ", top_emotions$sentiment_label[1], 
  ", making up ", round(top_emotions$percentage[1], 2), "% of the emotional content.\n",
  "Other significant sentiments include ", top_emotions$sentiment_label[2], 
  " (", round(top_emotions$percentage[2], 2), "%) and ", top_emotions$sentiment_label[3],
  " (", round(top_emotions$percentage[3], 2), "%)."
)

# 8. Voice Usage Summary
voice_usage_summary <- voice_usage %>%
  mutate(
    summary = paste0(voice, ": ", percentage, "%")
  ) %>%
  pull(summary) %>%
  paste(collapse = "\n")

# 9. Stylistic Features
average_sentence_length <- mean(narrative_structure_df$avg_sentence_length, na.rm = TRUE)
type_token_ratio <- length(unique(annotation_df$token)) / nrow(annotation_df)

# Generate Summary Text
summary_text <- paste0(
  "### Summary of the Telegram Channel\n\n",
  "**Total Posts Analyzed:** ", total_posts, "\n",
  "**Average Reactions per Post:** ", round(average_reactions, 2), "\n",
  "**Average Weighted Engagement per Post:** ", round(average_weighted_engagement, 2), "\n",
  "**Overall Total Engagement:** ", overall_engagement_total, "\n\n",
  "**Best Hours for Posting:**\n",
  paste0("- ", top_hours$hour, ": ", top_hours$posts_count, " posts\n", collapse = ""),
  "\n**Average Word Count per Post:** ", round(average_word_count, 2), "\n",
  "**Median Engagement Rate:** ", round(median_engagement_rate, 4), "\n",
  "**Correlation Between Total Reactions and Weighted Engagement:** ", round(correlation, 3), "\n\n",
  emotional_language_summary, "\n\n",
  "**Voice Usage in Text:**\n",
  voice_usage_summary, "\n\n",
  "**Stylistic Features:**\n",
  "- Average Sentence Length: ", round(average_sentence_length, 2), " words\n",
  "- Type-Token Ratio: ", round(type_token_ratio, 2), "\n"
)

cat(summary_text)

```

```{r}
# Word Cloud Visualization
library(wordcloud2)

# Prepare lemmas for word cloud
lemmas_filtered <- annotation_df %>%
  filter(upos %in% c("NOUN", "ADJ")) %>%  # Focus on nouns and adjectives
  pull(lemma) %>%                           # Extract the lemma column
  tolower() %>%                             # Convert to lowercase for consistency
  str_replace_all("[^–∞-—è—ñ—ó“ë—î—å']+", " ") %>% # Remove non-Cyrillic characters
  str_trim()                                # Trim whitespace

# Remove stopwords
lemmas_clean <- lemmas_filtered %>%
  str_split("\\s+") %>%  # Split into individual words
  unlist() %>%           # Flatten the list
  .[!. %in% ukrainian_stopwords]  # Remove stopwords

# Create a frequency table
word_freq <- table(lemmas_clean) %>%
  as.data.frame(stringsAsFactors = FALSE) %>%
  rename(word = lemmas_clean, freq = Freq) %>%
  arrange(desc(freq)) %>%
  top_n(100, freq)  # Select the top 100 most frequent words

# Generate the word cloud
wordcloud2(word_freq, size = 1, color = 'random-light', backgroundColor = "white")
```

```{r}
# Sentiment Distribution Pie Chart
ggplot(sentiment_long, aes(x = "", y = percentage, fill = sentiment_label)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar(theta = "y") +
  labs(title = "–†–æ–∑–ø–æ–¥—ñ–ª —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç—ñ–≤") +
  theme_void() +
  theme(legend.title = element_blank())

```

```{r}
# Readability Scores Boxplot
library(ggplot2)
library(tidyr)

readability_long <- readability_df %>%
  gather(key = "measure", value = "value", ARI, `Coleman.Liau.grade`, `Flesch.Kincaid`, SMOG)

ggplot(readability_long, aes(x = measure, y = value, fill = measure)) +
  geom_boxplot() +
  labs(title = "–†–æ–∑–ø–æ–¥—ñ–ª –ø–æ–∫–∞–∑–Ω–∏–∫—ñ–≤ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—ñ", x = "–ü–æ–∫–∞–∑–Ω–∏–∫ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—ñ", y = "–ë–∞–ª–∏") +
  theme_minimal() +
  theme(legend.position = "none")

```

```{r}
# Flesch-Kincaid Readability Score Histogram
ggplot(readability_df, aes(x = Flesch.Kincaid)) +
  geom_histogram(binwidth = 1, fill = "darkgreen", color = "black", alpha = 0.7) +
  labs(title = "–†–æ–∑–ø–æ–¥—ñ–ª –ø–æ–∫–∞–∑–Ω–∏–∫—ñ–≤ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—ñ Flesch-Kincaid", x = "Flesch-Kincaid Score", y = "–ß–∞—Å—Ç–æ—Ç–∞") +
  theme_minimal()

```

```{r}
# Top Terms per LDA Topic Visualization
library(ggplot2)

ggplot(top_terms_2, aes(x = reorder(term, beta), y = beta, fill = factor(topic))) +
  geom_bar(stat = "identity") +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  labs(title = "–¢–æ–ø —Ç–µ—Ä–º—ñ–Ω—ñ–≤ –∑–∞ —Ç–µ–º–∞–º–∏ LDA", x = "–¢–µ—Ä–º—ñ–Ω", y = "–ë–µ—Ç–∞") +
  theme_minimal() +
  theme(legend.position = "none")

```

```{r}
# ANOVA for Hour of Day on Engagement Rate
studfreedom_df <- studfreedom_df %>%
  mutate(hour = hour(normalized_datetime))  # Extract hour from datetime

anova_hour <- aov(engagement_rate ~ factor(hour), data = studfreedom_df)
anova_table_hour <- anova(anova_hour)

# Display ANOVA results
print(anova_table_hour)

# Evaluate significance
significance_result(anova_hour)

# Boxplot: Engagement Rate by Hour of Day
ggplot(studfreedom_df, aes(x = factor(hour), y = engagement_rate)) +
  geom_boxplot(fill = "skyblue") +
  labs(
    title = "–ó–∞–ª—É—á–µ–Ω—ñ—Å—Ç—å –∑–∞ –≥–æ–¥–∏–Ω–æ—é –¥–Ω—è",
    x = "–ì–æ–¥–∏–Ω–∞ –¥–Ω—è",
    y = "–ó–∞–ª—É—á–µ–Ω—ñ—Å—Ç—å (–†–µ–∞–∫—Ü—ñ—ó)"
  ) +
  theme_minimal()

```

```{r}
# ANOVA for Weekday on Engagement Rate
studfreedom_df <- studfreedom_df %>%
  mutate(weekday = wday(normalized_datetime, label = TRUE, abbr = FALSE, week_start = 1))  # Extract weekday

anova_weekday <- aov(engagement_rate ~ weekday, data = studfreedom_df)
anova_table_weekday <- anova(anova_weekday)

# Display ANOVA results
print(anova_table_weekday)

# Evaluate significance
significance_result(anova_weekday)

# Boxplot: Engagement Rate by Day of Week
ggplot(studfreedom_df, aes(x = weekday, y = engagement_rate)) +
  geom_boxplot(fill = "lightcoral") +
  labs(
    title = "–ó–∞–ª—É—á–µ–Ω—ñ—Å—Ç—å –∑–∞ –¥–Ω–µ–º —Ç–∏–∂–Ω—è",
    x = "–î–µ–Ω—å —Ç–∏–∂–Ω—è",
    y = "–ó–∞–ª—É—á–µ–Ω—ñ—Å—Ç—å (–†–µ–∞–∫—Ü—ñ—ó)"
  ) +
  theme_minimal()

```

```{r}
# POS Frequency by Section Visualization
pos_long <- pos_section_df %>%
  select(c("NOUN", "VERB", "ADJ", "ADV", "PRON")) %>% # Adjust based on available POS tags
  gather(key = "upos", value = "freq", -doc_id, -position)

ggplot(pos_long, aes(x = position, y = freq, fill = upos)) +
  geom_bar(stat = "identity", position = "stack") +
  labs(title = "–ß–∞—Å—Ç–æ—Ç–∞ POS –∑–∞ —Å–µ–∫—Ü—ñ—è–º–∏", x = "–°–µ–∫—Ü—ñ—è", y = "–ß–∞—Å—Ç–æ—Ç–∞") +
  theme_minimal()

```

```{r}
# Conjunctions Frequency Analysis

# Convert cohesion_df to tibble if it's a matrix
if (is.matrix(cohesion_df) || is.array(cohesion_df)) {
  cohesion_df <- as_tibble(as.data.frame(cohesion_df), .name_repair = "unique")
}

# Pivot conjunction counts to long format
conjunction_freq <- cohesion_df %>%
  pivot_longer(
    cols = everything(),
    names_to = "conjunction",
    values_to = "count"
  ) %>%
  group_by(conjunction) %>%
  summarise(
    total_count = sum(count, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  arrange(desc(total_count)) %>%
  slice_max(order_by = total_count, n = 20)

# View the top conjunctions
print(conjunction_freq)

# Plot Top 20 Conjunctions
ggplot(conjunction_freq, aes(x = reorder(conjunction, total_count), y = total_count)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  coord_flip() +
  labs(
    title = "–¢–æ–ø-20 –Ω–∞–π—á–∞—Å—Ç—ñ—à–∏—Ö —Å–ø–æ–ª—É—á–Ω–∏–∫—ñ–≤",
    x = "–°–ø–æ–ª—É—á–Ω–∏–∫",
    y = "–ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    axis.text = element_text(size = 10),
    axis.title = element_text(size = 12, face = "bold"),
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5)
  )

```

